{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca15fa0f",
   "metadata": {},
   "source": [
    "## TabDPT Evaluation Notebook \n",
    "\n",
    "This notebook to computes confidence intervals with `rliable` as in the paper appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rliable import library as rly\n",
    "from rliable import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the csvs from TabDPT results on at least 2 folds\n",
    "# This is important to compute confidence intervals with rliable\n",
    "tabdpt_df0 = pd.read_csv('../results_fold0.csv')\n",
    "tabdpt_df1 = pd.read_csv('../results_fold1.csv')\n",
    "\n",
    "tabdpt_df0['fold'] = 0\n",
    "tabdpt_df1['fold'] = 1\n",
    "tabdpt_df = pd.concat([tabdpt_df0, tabdpt_df1], axis=0)\n",
    "\n",
    "tabdpt_df['alg_name'] = 'TabDPT'\n",
    "\n",
    "df = tabdpt_df  # can join several tables with same structure and different alg_name\n",
    "\n",
    "# column cc18 is 1 when auc is not NaN, 0 otherwise\n",
    "df['cc18'] = df['auc'].notna().astype(int)\n",
    "\n",
    "# column ctr is 1 when r2 is not NaN, 0 otherwise\n",
    "df['ctr'] = df['r2'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table construction\n",
    "def get_scores_ci(metric, suite, data):\n",
    "    data_suite = data[data['cc18'] == 1] if suite == 'cc18' else data[data['ctr'] == 1]\n",
    "\n",
    "    algorithm_metric_dict = {}\n",
    "    for alg_name, group in data_suite.groupby('alg_name'):\n",
    "        # Create a pivot table: rows are folds, columns are datasets, values are r2 scores\n",
    "        pivot_table = group.pivot(index='fold', columns='name', values=metric)\n",
    "        scores =  pivot_table.values\n",
    "        # if there are NaN values, replace them with row mean\n",
    "        scores = np.where(np.isnan(scores), np.nanmean(scores, axis=1, keepdims=True), scores)\n",
    "        algorithm_metric_dict[alg_name] = scores\n",
    "    algorithms = list(algorithm_metric_dict.keys())\n",
    "\n",
    "    # choose one of the following aggregate functions\n",
    "    aggregate_func = lambda x: np.array([\n",
    "        # metrics.aggregate_median(x),\n",
    "        metrics.aggregate_iqm(x),\n",
    "        # metrics.aggregate_mean(x),\n",
    "        # metrics.aggregate_optimality_gap(x)\n",
    "    ])\n",
    "    aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n",
    "        algorithm_metric_dict, aggregate_func, reps=20000\n",
    "    )\n",
    "\n",
    "    aggregate_scores = {alg: aggregate_scores[alg] for alg in algorithms}\n",
    "    aggregate_score_cis = {alg: aggregate_score_cis[alg] for alg in algorithms}\n",
    "    return aggregate_scores, aggregate_score_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_is_higher = {\n",
    "    'auc':  True,\n",
    "    'acc':  True,\n",
    "    'corr': True,\n",
    "    'r2':   True,\n",
    "\n",
    "    'auc_rank': False,\n",
    "    'acc_rank': False,\n",
    "    'corr_rank': False,\n",
    "    'r2_rank': False,\n",
    "}\n",
    "\n",
    "metric_suite_pairs = [\n",
    "    ('auc',  'cc18'),  \n",
    "    ('acc',  'cc18'),  \n",
    "    ('corr', 'ctr'), \n",
    "    ('r2',   'ctr'),  \n",
    "]\n",
    "# add _rank to the metrics\n",
    "# you can uncomment this to compute ranks instead of raw scores but you need more than one alg_name\n",
    "# metric_suite_pairs = [(m + \"_rank\", s) for m, s in metric_suite_pairs]\n",
    "\n",
    "all_scores = {} \n",
    "all_cis    = {} \n",
    "algorithms = set()\n",
    "\n",
    "for metric, suite in metric_suite_pairs:\n",
    "    scores, ci = get_scores_ci(metric, suite, df) \n",
    "    scores = {k: float(v.squeeze()) for k, v in scores.items()}\n",
    "    ci     = {k: tuple(v.squeeze()) for k, v in ci.items()}\n",
    "    all_scores[(metric, suite)] = scores\n",
    "    all_cis[(metric, suite)]    = ci\n",
    "    algorithms.update(scores.keys())\n",
    "\n",
    "algorithms = sorted(algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabdpt-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
